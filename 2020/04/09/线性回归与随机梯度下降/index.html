<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>线性回归与随机梯度下降 | ThinkDeeper</title><meta name="description" content><meta name="keywords" content="Machine Learning"><meta name="author" content="WJZheng"><meta name="copyright" content="WJZheng"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="线性回归与随机梯度下降"><meta name="twitter:description" content><meta name="twitter:image" content="https://wellenzheng.github.io/img/0119.jpg"><meta property="og:type" content="article"><meta property="og:title" content="线性回归与随机梯度下降"><meta property="og:url" content="https://wellenzheng.github.io/2020/04/09/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"><meta property="og:site_name" content="ThinkDeeper"><meta property="og:description" content><meta property="og:image" content="https://wellenzheng.github.io/img/0119.jpg"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://wellenzheng.github.io/2020/04/09/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"><link rel="prev" title="逻辑回归和支持向量机" href="https://wellenzheng.github.io/2020/04/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%92%8C%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"><link rel="next" title="二手车交易价格预测-建模调参" href="https://wellenzheng.github.io/2020/03/31/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B-%E5%BB%BA%E6%A8%A1%E8%B0%83%E5%8F%82/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5/js/md5.min.js"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: true,
  copyright: {"languages":{"author":"Author: WJZheng","link":"Link: ","source":"Source: ThinkDeeper","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"] > svg a {
  fill: blue;
  stroke: blue;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container {
  overflow: auto hidden;
}

mjx-container + br {
  display: none;
}
</style><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">49</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">11</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">9</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li><li><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-picture-o"></i><span> Picture</span></a></li></ul></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#实验目的"><span class="toc-number">1.</span> <span class="toc-text">实验目的</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#实验数据"><span class="toc-number">2.</span> <span class="toc-text">实验数据</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#实验步骤"><span class="toc-number">3.</span> <span class="toc-text">实验步骤</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#代码示例"><span class="toc-number">4.</span> <span class="toc-text">代码示例</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#导入依赖"><span class="toc-number">4.1.</span> <span class="toc-text">导入依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#读取数据"><span class="toc-number">4.2.</span> <span class="toc-text">读取数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#划分训练集与验证集"><span class="toc-number">4.3.</span> <span class="toc-text">划分训练集与验证集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型参数初始化"><span class="toc-number">4.4.</span> <span class="toc-text">模型参数初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义loss函数"><span class="toc-number">4.5.</span> <span class="toc-text">定义loss函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#为X添加偏移量"><span class="toc-number">4.6.</span> <span class="toc-text">为X添加偏移量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#查看当前训练集的loss"><span class="toc-number">4.7.</span> <span class="toc-text">查看当前训练集的loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#闭式解"><span class="toc-number">4.8.</span> <span class="toc-text">闭式解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#定义闭式解函数"><span class="toc-number">4.8.1.</span> <span class="toc-text">定义闭式解函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#求出闭式解"><span class="toc-number">4.8.2.</span> <span class="toc-text">求出闭式解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#闭式解在训练集下的loss"><span class="toc-number">4.8.3.</span> <span class="toc-text">闭式解在训练集下的loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#闭式解在验证集下的loss"><span class="toc-number">4.8.4.</span> <span class="toc-text">闭式解在验证集下的loss</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降"><span class="toc-number">4.9.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#定义梯度函数"><span class="toc-number">4.9.1.</span> <span class="toc-text">定义梯度函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#定义下降函数"><span class="toc-number">4.9.2.</span> <span class="toc-text">定义下降函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#全批量梯度下降"><span class="toc-number">4.9.3.</span> <span class="toc-text">全批量梯度下降</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#尝试Adam、Momentum、RMSprop、SGD、Mini-Batch等算法"><span class="toc-number">4.10.</span> <span class="toc-text">尝试Adam、Momentum、RMSprop、SGD、Mini-Batch等算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#读取新数据集"><span class="toc-number">4.10.1.</span> <span class="toc-text">读取新数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#定义随机梯度下降函数"><span class="toc-number">4.10.2.</span> <span class="toc-text">定义随机梯度下降函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-batch和SGD的比较"><span class="toc-number">4.10.3.</span> <span class="toc-text">Mini-batch和SGD的比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-batch和Momentum的比较"><span class="toc-number">4.10.4.</span> <span class="toc-text">Mini-batch和Momentum的比较</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mini-batch和RMSprop的比较"><span class="toc-number">4.11.</span> <span class="toc-text">Mini-batch和RMSprop的比较</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-batch和Adam的比较"><span class="toc-number">4.11.1.</span> <span class="toc-text">Mini-batch和Adam的比较</span></a></li></ol></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(/img/0119.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">ThinkDeeper</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li><li><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-picture-o"></i><span> Picture</span></a></li></ul></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">线性回归与随机梯度下降</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="Created 2020-04-09 17:18:03"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2020-04-09</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="Updated 2020-04-09 18:27:31"><i class="fa fa-history" aria-hidden="true"></i> Updated 2020-04-09</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Machine-Learning/">Machine Learning</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>Post View:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>Comments:</span><a href="/2020/04/09/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><h1 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h1><hr>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><ol>
<li>进一步理解线性回归，闭式解和随机梯度下降的原理。</li>
<li>在小规模数据集上实践。</li>
<li>体会优化和调参的过程。</li>
</ol>
<h1 id="实验数据"><a href="#实验数据" class="headerlink" title="实验数据"></a>实验数据</h1><hr>
<p>线性回归使用的是<a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/" target="_blank" rel="noopener">LIBSVM Data</a>中的<a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html#housing" target="_blank" rel="noopener">Housing</a>数据，包含506个样本，每个样本有13个属性。请自行下载scaled版本，并将其切分为训练集，验证集。</p>
<h1 id="实验步骤"><a href="#实验步骤" class="headerlink" title="实验步骤"></a>实验步骤</h1><p><em>线性回归的闭式解</em></p>
<ol>
<li>读取实验数据，使用sklearn库的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_file.html" target="_blank" rel="noopener">load_svmlight_file</a>函数读取数据。</li>
<li>将数据集切分为训练集和验证集，本次实验不切分测试集。使用<a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener">train_test_split</a>函数切分数据集。</li>
<li>选取一个Loss函数。</li>
<li>获取闭式解的公式，过程详⻅课件ppt。</li>
<li>通过闭式解得到参数W的值。</li>
<li>在训练集上测试并获得Loss函数值loss_train，在验证集上获得Loss函数值。</li>
<li>输出Loss，Loss_train和loss_val的值。</li>
</ol>
<p><em>线性回归和随机梯度下降</em></p>
<ol>
<li>读取实验数据，使用sklearn库的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_file.html" target="_blank" rel="noopener">load_svmlight_file</a>函数读取数据。</li>
<li>将数据集切分为训练集和验证集，本次实验不切分测试集。使用<a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener">train_test_split</a>函数切分数据集。</li>
<li>线性模型参数初始化，可以考虑全零初始化，随机初始化或者正态分布初始化。</li>
<li>选择Loss函数及对其求导，过程详见课件ppt。</li>
<li>随机选取训练集中的一个样本，求得该样本对函数的梯度。</li>
<li>取梯度的负方向，记为。</li>
<li>更新模型参数，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex" xmlns="http://www.w3.org/2000/svg" width="16.718ex" height="2.034ex" role="img" focusable="false" viewBox="0 -683 7389.2 899" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-57"></use></g><g data-mml-node="mi" transform="translate(944, -150) scale(0.707)"><use xlink:href="#MJX-TEX-I-74"></use></g></g><g data-mml-node="mo" transform="translate(1527, 0)"><use xlink:href="#MJX-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(2582.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-57"></use></g><g data-mml-node="TeXAtom" transform="translate(944, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-74"></use></g><g data-mml-node="mo" transform="translate(361, 0)"><use xlink:href="#MJX-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(1139, 0)"><use xlink:href="#MJX-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(4958, 0)"><use xlink:href="#MJX-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(5958.2, 0)"><use xlink:href="#MJX-TEX-I-3BC"></use></g><g data-mml-node="mi" transform="translate(6561.2, 0)"><use xlink:href="#MJX-TEX-I-44"></use></g></g></g></svg></mjx-container>。<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex" xmlns="http://www.w3.org/2000/svg" width="1.364ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 603 658" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3BC"></use></g></g></g></svg></mjx-container>为学习率，是人为调整的超参数。</li>
<li>在训练集上测试并得到Loss函数值loss_train，在验证集上测试并得到Loss函数值loss_val。</li>
<li>重复步骤5-8若干次，输出loss_train和loss_val的值。</li>
</ol>
<h1 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h1><hr>
<h2 id="导入依赖"><a href="#导入依赖" class="headerlink" title="导入依赖"></a>导入依赖</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets <span class="keyword">as</span> sd</span><br><span class="line"><span class="keyword">import</span> sklearn.model_selection <span class="keyword">as</span> sms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>

<h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取实验数据</span></span><br><span class="line">X, y = sd.load_svmlight_file(<span class="string">'housing_scale.txt'</span>,n_features = <span class="number">13</span>)</span><br></pre></td></tr></table></figure>

<h2 id="划分训练集与验证集"><a href="#划分训练集与验证集" class="headerlink" title="划分训练集与验证集"></a>划分训练集与验证集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数据集切分为训练集和验证集</span></span><br><span class="line">X_train, X_valid, y_train, y_valid = sms.train_test_split(X, y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将稀疏矩阵转为ndarray类型</span></span><br><span class="line">X_train = X_train.toarray()</span><br><span class="line">X_valid = X_valid.toarray()</span><br><span class="line">y_train = y_train.reshape(len(y_train),<span class="number">1</span>)</span><br><span class="line">y_valid = y_valid.reshape(len(y_valid),<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train.shape, X_valid.shape, y_train.shape, y_valid.shape</span><br></pre></td></tr></table></figure>

<p><img src="/" class="lazyload" data-src="http://q7yezbh4o.bkt.clouddn.com/image/0092.PNG"  alt=""></p>
<h2 id="模型参数初始化"><a href="#模型参数初始化" class="headerlink" title="模型参数初始化"></a>模型参数初始化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性模型参数初始化，可以考虑全零初始化，随机初始化或者正态分布初始化。</span></span><br><span class="line">theta = np.zeros((<span class="number">14</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<h2 id="定义loss函数"><a href="#定义loss函数" class="headerlink" title="定义loss函数"></a>定义loss函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选取一个Loss函数，计算训练集的Loss函数值，记为loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    hx = X.dot(theta)</span><br><span class="line">    error = np.power((hx - y), <span class="number">2</span>).mean() / <span class="number">2</span></span><br><span class="line"><span class="comment">#     reg = np.power(theta[1:theta.shape[0]],2).mean()</span></span><br><span class="line">    <span class="keyword">return</span> error</span><br></pre></td></tr></table></figure>

<h2 id="为X添加偏移量"><a href="#为X添加偏移量" class="headerlink" title="为X添加偏移量"></a>为X添加偏移量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train = np.concatenate((np.ones((X_train.shape[<span class="number">0</span>],<span class="number">1</span>)), X_train), axis = <span class="number">1</span>)</span><br><span class="line">X_valid = np.concatenate((np.ones((X_valid.shape[<span class="number">0</span>],<span class="number">1</span>)), X_valid), axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train.shape, X_valid.shape</span><br></pre></td></tr></table></figure>

<p><img src="/" class="lazyload" data-src="http://q7yezbh4o.bkt.clouddn.com/image/0093.PNG"  alt=""></p>
<h2 id="查看当前训练集的loss"><a href="#查看当前训练集的loss" class="headerlink" title="查看当前训练集的loss"></a>查看当前训练集的loss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = compute_loss(X_train, y_train, theta)</span><br><span class="line">loss</span><br></pre></td></tr></table></figure>

<p><img src="/" class="lazyload" data-src="http://q7yezbh4o.bkt.clouddn.com/image/0094.PNG"  alt=""></p>
<h2 id="闭式解"><a href="#闭式解" class="headerlink" title="闭式解"></a>闭式解</h2><h3 id="定义闭式解函数"><a href="#定义闭式解函数" class="headerlink" title="定义闭式解函数"></a>定义闭式解函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 闭式解函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_equation</span><span class="params">(X, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (np.linalg.inv(X.T.dot(X))).dot(X.T).dot(y)</span><br></pre></td></tr></table></figure>

<h3 id="求出闭式解"><a href="#求出闭式解" class="headerlink" title="求出闭式解"></a>求出闭式解</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta = normal_equation(X_train, y_train)</span><br><span class="line">theta</span><br></pre></td></tr></table></figure>

<p><img src="/" class="lazyload" data-src="http://q7yezbh4o.bkt.clouddn.com/image/0095.PNG"  alt=""></p>
<h3 id="闭式解在训练集下的loss"><a href="#闭式解在训练集下的loss" class="headerlink" title="闭式解在训练集下的loss"></a>闭式解在训练集下的loss</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_train = compute_loss(X_train, y_train, theta)</span><br><span class="line">loss_train</span><br></pre></td></tr></table></figure>

<p><img src="/" class="lazyload" data-src="http://q7yezbh4o.bkt.clouddn.com/image/0096.PNG"  alt=""></p>
<h3 id="闭式解在验证集下的loss"><a href="#闭式解在验证集下的loss" class="headerlink" title="闭式解在验证集下的loss"></a>闭式解在验证集下的loss</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_valid = compute_loss(X_valid, y_valid, theta)</span><br><span class="line">loss_valid</span><br></pre></td></tr></table></figure>

<p><img src="/" class="lazyload" data-src="http://q7yezbh4o.bkt.clouddn.com/image/0097.PNG"  alt=""></p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h3 id="定义梯度函数"><a href="#定义梯度函数" class="headerlink" title="定义梯度函数"></a>定义梯度函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> X.T.dot(X.dot(theta) - y)</span><br></pre></td></tr></table></figure>

<h3 id="定义下降函数"><a href="#定义下降函数" class="headerlink" title="定义下降函数"></a>定义下降函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">descent</span><span class="params">(X, y, theta, alpha, iters, X_valid, y_valid)</span>:</span></span><br><span class="line">    loss_train = np.zeros((iters,<span class="number">1</span>))</span><br><span class="line">    loss_valid = np.zeros((iters,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">        grad = gradient(X, y, theta)</span><br><span class="line">        theta = theta - alpha * grad</span><br><span class="line">        loss_train[i] = compute_loss(X, y, theta)</span><br><span class="line">        loss_valid[i] = compute_loss(X_valid, y_valid, theta)</span><br><span class="line">    <span class="keyword">return</span> theta, loss_train, loss_valid</span><br></pre></td></tr></table></figure>

<h3 id="全批量梯度下降"><a href="#全批量梯度下降" class="headerlink" title="全批量梯度下降"></a>全批量梯度下降</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 全批量梯度下降</span></span><br><span class="line">theta = np.zeros((<span class="number">14</span>,<span class="number">1</span>))</span><br><span class="line">alpha = <span class="number">0.001</span></span><br><span class="line">iters = <span class="number">25</span></span><br><span class="line">opt_theta, loss_train, loss_valid = descent(X_train, y_train, theta, alpha, iters, X_valid, y_valid)</span><br><span class="line">loss_train.min(), loss_valid.min()</span><br></pre></td></tr></table></figure>

<p><img src="/" class="lazyload" data-src="http://q7yezbh4o.bkt.clouddn.com/image/0098.PNG"  alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">iteration = np.arange(<span class="number">0</span>, iters, step = <span class="number">1</span>)</span><br><span class="line">fig, ax = plt.subplots(figsize = (<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.set_title(<span class="string">'Train'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'iteration'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.plot(iteration, loss_train, <span class="string">'b'</span>, label=<span class="string">'Train'</span>)</span><br><span class="line"><span class="comment"># plt.plot(iteration, loss_valid, 'r', label='Valid')</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/" class="lazyload" data-src="http://q7yezbh4o.bkt.clouddn.com/image/0099.PNG"  alt=""></p>
<h2 id="尝试Adam、Momentum、RMSprop、SGD、Mini-Batch等算法"><a href="#尝试Adam、Momentum、RMSprop、SGD、Mini-Batch等算法" class="headerlink" title="尝试Adam、Momentum、RMSprop、SGD、Mini-Batch等算法"></a>尝试Adam、Momentum、RMSprop、SGD、Mini-Batch等算法</h2><h3 id="读取新数据集"><a href="#读取新数据集" class="headerlink" title="读取新数据集"></a>读取新数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这是一个新的数据，以验证Adam方法的效果</span></span><br><span class="line">X, y = sd.load_svmlight_file(<span class="string">'cpusmall_scale.txt'</span>,n_features = <span class="number">13</span>)</span><br><span class="line"><span class="comment"># 将数据集切分为训练集和验证集</span></span><br><span class="line">X_train, X_valid, y_train, y_valid = sms.train_test_split(X, y)</span><br><span class="line"><span class="comment"># 将稀疏矩阵转为ndarray类型</span></span><br><span class="line">X_train = X_train.toarray()</span><br><span class="line">X_valid = X_valid.toarray()</span><br><span class="line">y_train = y_train.reshape(len(y_train),<span class="number">1</span>)</span><br><span class="line">y_valid = y_valid.reshape(len(y_valid),<span class="number">1</span>)</span><br><span class="line">X_train = np.concatenate((np.ones((X_train.shape[<span class="number">0</span>],<span class="number">1</span>)), X_train), axis = <span class="number">1</span>)</span><br><span class="line">X_valid = np.concatenate((np.ones((X_valid.shape[<span class="number">0</span>],<span class="number">1</span>)), X_valid), axis = <span class="number">1</span>)</span><br><span class="line">X_train.shape, X_valid.shape, y_train.shape, y_valid.shape</span><br></pre></td></tr></table></figure>

<p><img src="/" class="lazyload" data-src="http://q7yezbh4o.bkt.clouddn.com/image/0100.PNG"  alt=""></p>
<h3 id="定义随机梯度下降函数"><a href="#定义随机梯度下降函数" class="headerlink" title="定义随机梯度下降函数"></a>定义随机梯度下降函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机梯度下降的执行函数 batch_size = 1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stochastic_descent</span><span class="params">(X, y, theta, alpha, iters, batch_size, X_valid, y_valid, opt = <span class="string">'None'</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 参数初始化</span></span><br><span class="line">    <span class="comment"># beta1 为 Momentum参数，值越大，则之前的梯度对现在的方向影响越大</span></span><br><span class="line">    <span class="comment"># beta2 为 RMSprop的衰减速率， epsilon防止分母为0</span></span><br><span class="line">    data = pd.DataFrame(np.concatenate((y.reshape(y.size,<span class="number">1</span>),X), axis = <span class="number">1</span>))</span><br><span class="line">    loss_train = np.zeros((iters,<span class="number">1</span>))</span><br><span class="line">    loss_valid = np.zeros((iters,<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    v1= np.zeros((X.shape[<span class="number">1</span>],<span class="number">1</span>))</span><br><span class="line">    v2= np.zeros((X.shape[<span class="number">1</span>],<span class="number">1</span>))</span><br><span class="line">    beta1 = <span class="number">0.9</span></span><br><span class="line">    beta2 = <span class="number">0.999</span></span><br><span class="line">    epsilon = <span class="number">1e-8</span></span><br><span class="line">    t = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">        batch = data.sample(batch_size, replace=<span class="literal">True</span>)</span><br><span class="line">        batch = batch.values</span><br><span class="line">        data_X = batch[:,<span class="number">1</span>:theta.size+<span class="number">1</span>]</span><br><span class="line">        data_y = batch[:,<span class="number">0</span>]</span><br><span class="line"> </span><br><span class="line">        grad = gradient(data_X, data_y, theta)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(opt==<span class="string">'Momentum'</span>):</span><br><span class="line">            v1 = beta1 * v1 + (<span class="number">1</span>-beta1) * grad</span><br><span class="line">            theta = theta - alpha * v1</span><br><span class="line">        <span class="keyword">elif</span>(opt==<span class="string">'RMSprop'</span>):</span><br><span class="line">            v2 = beta2 * v2 + (<span class="number">1</span>-beta2) * grad * grad</span><br><span class="line">            theta = theta - alpha * grad/(np.sqrt(v2+epsilon))</span><br><span class="line">        <span class="keyword">elif</span>(opt==<span class="string">'Adam'</span>):</span><br><span class="line">            v1 = beta1 * v1 + (<span class="number">1</span>-beta1) * grad</span><br><span class="line">            v2 = beta2 * v2 + (<span class="number">1</span>-beta2) * grad * grad</span><br><span class="line"><span class="comment">#             v1 = v1 / (1 - np.power(beta1, t))</span></span><br><span class="line"><span class="comment">#             v2 = v2 / (1 - np.power(beta2, t))</span></span><br><span class="line">            t = t + <span class="number">1</span>   </span><br><span class="line">            theta = theta - alpha * v1/(np.sqrt(v2)+epsilon)</span><br><span class="line">        <span class="keyword">elif</span>(opt==<span class="string">'None'</span>):</span><br><span class="line">            theta = theta - alpha * grad</span><br><span class="line">        </span><br><span class="line">        loss_train[i] = compute_loss(X, y, theta)</span><br><span class="line">    <span class="keyword">return</span> theta, loss_train</span><br></pre></td></tr></table></figure>

<h3 id="Mini-batch和SGD的比较"><a href="#Mini-batch和SGD的比较" class="headerlink" title="Mini-batch和SGD的比较"></a>Mini-batch和SGD的比较</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.001</span></span><br><span class="line">iters = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机梯度下降</span></span><br><span class="line">SGD_theta = np.zeros((<span class="number">14</span>,<span class="number">1</span>))</span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">SGD_theta, SGD_train = stochastic_descent(X_train, y_train, SGD_theta, alpha, iters, batch_size, X_valid, y_valid)</span><br><span class="line">SGD_valid = compute_loss(X_valid, y_valid, SGD_theta)</span><br><span class="line">print(SGD_train.min())</span><br><span class="line">print(SGD_valid)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 批量梯度下降</span></span><br><span class="line">Mini_theta = np.zeros((<span class="number">14</span>,<span class="number">1</span>))</span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"></span><br><span class="line">Mini_theta, Mini_train = stochastic_descent(X_train, y_train, Mini_theta, alpha, iters, batch_size, X_valid, y_valid)</span><br><span class="line">Mini_valid = compute_loss(X_valid, y_valid, Mini_theta)</span><br><span class="line">print(Mini_train.min())</span><br><span class="line">print(Mini_valid)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">iteration = np.arange(<span class="number">0</span>, iters, step = <span class="number">1</span>)</span><br><span class="line">fig, ax = plt.subplots(figsize = (<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.set_title(<span class="string">'SGD vs MiniBatch'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'iteration'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.plot(iteration, SGD_train,<span class="string">'b'</span>, label=<span class="string">'SGD_train'</span>)</span><br><span class="line">plt.plot(iteration, Mini_train,<span class="string">'r'</span>, label=<span class="string">'Mini_train'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/" class="lazyload" data-src="http://q7yezbh4o.bkt.clouddn.com/image/0101.PNG"  alt=""></p>
<h3 id="Mini-batch和Momentum的比较"><a href="#Mini-batch和Momentum的比较" class="headerlink" title="Mini-batch和Momentum的比较"></a>Mini-batch和Momentum的比较</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.0001</span></span><br><span class="line">iters = <span class="number">1000</span></span><br><span class="line">Mini_theta = np.zeros((<span class="number">14</span>,<span class="number">1</span>))</span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"></span><br><span class="line">Mini_theta, Mini_train = stochastic_descent(X_train, y_train, Mini_theta, alpha, iters, batch_size, X_valid, y_valid)</span><br><span class="line">Mini_valid = compute_loss(X_valid, y_valid, Mini_theta)</span><br><span class="line">print(Mini_train.min())</span><br><span class="line">print(Mini_valid)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Momentum_theta = np.zeros((<span class="number">14</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">Momentum_theta, Momentum_train = stochastic_descent(X_train, y_train, Momentum_theta, alpha, iters, batch_size, X_valid, y_valid, <span class="string">'Momentum'</span>)</span><br><span class="line">Momentum_valid = compute_loss(X_valid, y_valid, Momentum_theta)</span><br><span class="line">print(Momentum_train.min())</span><br><span class="line">print(Momentum_valid)</span><br><span class="line"></span><br><span class="line">right = iters</span><br><span class="line">iteration = np.arange(<span class="number">0</span>, right, step = <span class="number">1</span>)</span><br><span class="line">fig, ax = plt.subplots(figsize = (<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.set_title(<span class="string">'Momentum vs MiniBatch'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'iteration'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.plot(iteration, Momentum_train[<span class="number">0</span>:right],<span class="string">'b'</span>, label=<span class="string">'Momentum_train'</span>)</span><br><span class="line">plt.plot(iteration, Mini_train[<span class="number">0</span>:right],<span class="string">'r'</span>, label=<span class="string">'Mini_train'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/" class="lazyload" data-src="http://q7yezbh4o.bkt.clouddn.com/image/0102.PNG"  alt=""></p>
<h2 id="Mini-batch和RMSprop的比较"><a href="#Mini-batch和RMSprop的比较" class="headerlink" title="Mini-batch和RMSprop的比较"></a>Mini-batch和RMSprop的比较</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">iters = <span class="number">500</span></span><br><span class="line"></span><br><span class="line">RMSprop_theta = np.zeros((<span class="number">14</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">RMSprop_theta,RMSprop_train = stochastic_descent(X_train, y_train, RMSprop_theta, alpha, iters, batch_size, X_valid, y_valid, <span class="string">'RMSprop'</span>)</span><br><span class="line">RMSprop_valid = compute_loss(X_valid, y_valid, RMSprop_theta)</span><br><span class="line">print(RMSprop_train.min())</span><br><span class="line">print(RMSprop_valid)</span><br><span class="line"></span><br><span class="line">right = iters</span><br><span class="line">iteration = np.arange(<span class="number">0</span>, right, step = <span class="number">1</span>)</span><br><span class="line">fig, ax = plt.subplots(figsize = (<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.set_title(<span class="string">'RMSprop vs MiniBatch'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'iteration'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.plot(iteration, RMSprop_train[<span class="number">0</span>:right],<span class="string">'b'</span>, label=<span class="string">'RMSprop_train'</span>)</span><br><span class="line">plt.plot(iteration, Mini_train[<span class="number">0</span>:right],<span class="string">'r'</span>, label=<span class="string">'Mini_train'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/" class="lazyload" data-src="http://q7yezbh4o.bkt.clouddn.com/image/0103.PNG"  alt=""></p>
<h3 id="Mini-batch和Adam的比较"><a href="#Mini-batch和Adam的比较" class="headerlink" title="Mini-batch和Adam的比较"></a>Mini-batch和Adam的比较</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">iters = <span class="number">500</span></span><br><span class="line">Adam_theta = np.zeros((<span class="number">14</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">Adam_theta,Adam_train = stochastic_descent(X_train, y_train, Adam_theta, alpha, iters, batch_size, X_valid, y_valid, <span class="string">'Adam'</span>)</span><br><span class="line">Adam_valid = compute_loss(X_valid, y_valid, Adam_theta)</span><br><span class="line">print(Adam_train.min())</span><br><span class="line"></span><br><span class="line">print(Adam_valid)</span><br><span class="line"></span><br><span class="line">right = iters</span><br><span class="line">iteration = np.arange(<span class="number">0</span>, right, step = <span class="number">1</span>)</span><br><span class="line">fig, ax = plt.subplots(figsize = (<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.set_title(<span class="string">'Adam vs MiniBatch'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'iteration'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.plot(iteration, Adam_train[<span class="number">0</span>:right],<span class="string">'b'</span>, label=<span class="string">'Adam_train'</span>)</span><br><span class="line">plt.plot(iteration, Mini_train[<span class="number">0</span>:right],<span class="string">'r'</span>, label=<span class="string">'Mini_train'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/" class="lazyload" data-src="http://q7yezbh4o.bkt.clouddn.com/image/0104.PNG"  alt=""></p>
<svg style="display: none" id="MJX-SVG-global-cache"><defs><path id="MJX-TEX-I-57" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-TEX-I-74" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path id="MJX-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-TEX-I-3BC" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-TEX-I-44" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></defs></svg></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">WJZheng</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wellenzheng.github.io/2020/04/09/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">https://wellenzheng.github.io/2020/04/09/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine Learning</a></div><div class="post_share"><div class="social-share" data-image="/img/0010.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/04/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%92%8C%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"><img class="prev_cover lazyload" data-src="/img/0113.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">逻辑回归和支持向量机</div></div></a></div><div class="next-post pull_right"><a href="/2020/03/31/%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B-%E5%BB%BA%E6%A8%A1%E8%B0%83%E5%8F%82/"><img class="next_cover lazyload" data-src="/img/0138.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">二手车交易价格预测-建模调参</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/04/13/基于AdaBoost的人脸识别算法/" title="基于AdaBoost的人脸识别算法"><img class="relatedPosts_cover lazyload"data-src="/img/0054.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-13</div><div class="relatedPosts_title">基于AdaBoost的人脸识别算法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/13/基于矩阵分解的推荐系统/" title="基于矩阵分解的推荐系统"><img class="relatedPosts_cover lazyload"data-src="/img/0131.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-13</div><div class="relatedPosts_title">基于矩阵分解的推荐系统</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/09/逻辑回归和支持向量机/" title="逻辑回归和支持向量机"><img class="relatedPosts_cover lazyload"data-src="/img/0113.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-09</div><div class="relatedPosts_title">逻辑回归和支持向量机</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> Comment</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '31cf2294bcfe0aa35211',
  clientSecret: '608a10d0382cfe3193c44734a501c603b180743f',
  repo: 'gittalk',
  owner: 'wellenzheng',
  admin: ['wellenzheng'],
  id: md5(decodeURI(location.pathname)),
  language: 'en',
  perPage: 10,
  distractionFreeMode: false,
  pagerDirection: 'last',
  createIssueManually: false,
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  try {
    document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
  } catch (e) {
    return false
  }
}</script></div></article></main><footer id="footer" style="background-image: url(/img/0119.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 By WJZheng</div><div class="framework-info"><span>Driven </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="Scroll to comment"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script id="ribbon_piao" mobile="false" src="/js/third-party/piao.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js"></script><script>document.addEventListener('DOMContentLoaded', function() {
    pangu.spacingElementById('content-inner')
})</script><script src="/js/search/local-search.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>